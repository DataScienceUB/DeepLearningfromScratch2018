{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>June 2016 - This notebook was created by [Oriol Pujol Vila](http://www.maia.ub.es/~oriol). Source and [license](./LICENSE.txt) info are in the folder.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the problem up to this point. Let us recall the three basic components of the algorithm. \n",
    "\n",
    "+ **The model class** \n",
    "+ **The loss function**\n",
    "+ **The optimization algorithm**\n",
    "\n",
    "Let us consider the following graph representation of the problem:\n",
    "<img src = \"images/pipeline1.png\"  width = \"200\">\n",
    "\n",
    "The model function is $f(x,\\omega)$, where $\\omega$ are the parameters to optimise. For example, in the case of a linear model we may have $f(x,\\omega) = \\sum_i \\omega_i x_i = \\omega^T x$. The loss function is represented with $\\mathcal{L}(y,t)$, where $y=f(x,\\omega)$ and $t$ is the target value. Remember that the loss function models the dissimilarity between the output of the model and the true value to predict. The last element of the learning algorithm is the optimisation algorithm. In our case is an algorithm with the goal of minimizing the dissimilarity between target value and the model output, i.e.\n",
    "\n",
    "$$\\underset{\\omega}{\\text{minimize}} \\quad \\mathcal{L}(f(x,\\omega),t)$$\n",
    "\n",
    "Let us refactor the code using OOP in order to take into account these three elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,x):\n",
    "        #Takes a data point and evaluates f(x,w)\n",
    "        pass\n",
    "\n",
    "class loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def evaluate(self,x,t):\n",
    "        #Evaluates the loss function L(y,t)\n",
    "        pass\n",
    "\n",
    "class optimize:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def run(self, data, target, model, loss):\n",
    "        #Takes a loss function and a model and find the optimal parameters for an specific data set\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know one way of optimising algorithms in front of large scale data sets, i.e. stochastic subgradient methods. In short this is an iterative algorithm that updates the unknown variables proportional to minus the gradient magnitude at each iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fill in the gaps for a classical linear regression where the model is $f(x,\\omega) = \\omega^Tx$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class model:\n",
    "    def __init__(self,w):\n",
    "        self.w = w\n",
    "    def forward(self,x):\n",
    "        #Takes a data point and evaluates f(x,w)\n",
    "        return np.dot(self.w,x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using a least squares loss function $\\mathcal{L}(y,t) = (t-y)^2$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def evaluate(self,model,x,t):\n",
    "        #Evaluates the loss function L(y,t)\n",
    "        y = model.forward(x)\n",
    "        return (t-y)*(t-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using stochastic gradient descend,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class optimize:\n",
    "    def __init__(self,t):\n",
    "        self.num_iter = t\n",
    "\n",
    "    def run(self, data, target, model, loss):\n",
    "        #Takes a loss function and a model and find the optimal parameters for an specific data set\n",
    "        N_samples = data.shape[0]\n",
    "        for t in xrange(self.num_iter):\n",
    "            #Step 1.-take a sample x at random from the training set\n",
    "            idx = np.random.randint(N_samples)\n",
    "            xi = data[idx,:]\n",
    "            yi = target[idx]\n",
    "            \n",
    "            #Step 2.- update the parameters to optimise\n",
    "            model.w = model.w - eta * \"gradient_L_omega(xi,yi)\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient with respect to the parameter to optimise is $$\\nabla_\\omega\\mathcal{L(f(x,\\omega),t)} = (\\frac{\\partial \\mathcal{L}}{\\partial \\omega_1},\\frac{\\partial \\mathcal{L}}{\\partial \\omega_2}, \\dots, \\frac{\\partial \\mathcal{L}}{\\partial \\omega_d}) = \\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\omega}}$$.\n",
    "\n",
    "Recall the graph representation \n",
    "<img src = \"images\\pipeline1.png\"  width = \"200\">\n",
    "and consider the chain rule\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\omega}} = \\frac{\\partial \\mathcal{L}}{\\partial y}\\frac{\\partial y}{\\partial \\bar{\\omega}}$$\n",
    "\n",
    "Observe that the complete differentiation is easier this way. \n",
    "\n",
    "Given that $\\mathcal{L}(y,t) = (t-y)^2$, then \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial y} = -2(t-y)$$\n",
    "\n",
    "Given that $y = \\omega^Tx$, then \n",
    "$$ \\frac{\\partial y}{\\partial \\bar{\\omega}} = x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The flow of the chain rule\n",
    "\n",
    "Let us try to understand visually what the chain rule is about. This will constitute the basis of the backpropagation algorithm, that will be next used to train deep learning techniques and the basis of **automatic differentiation**, a programatically optimum way for computing differentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational graph\n",
    "\n",
    "Consider the following function $f(x,y,z) = (x+y)\\cdot z$. The computational graph corresponds to a graphical representation of the operations.\n",
    "\n",
    "<img src = \"images\\comp_graph1.jpg\"  width = \"300\">\n",
    "\n",
    "Each of the archs can be labelled. This lets us to introduce a midpoint $q$.\n",
    "\n",
    "Let us now look at the partial derivatives of each node with respect of its inputs. We have \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial f} = 1$$\n",
    "$$\\frac{\\partial f}{\\partial q} = z$$\n",
    "$$\\frac{\\partial f}{\\partial z} = q$$\n",
    "$$\\frac{\\partial q}{\\partial x} = 1$$\n",
    "$$\\frac{\\partial q}{\\partial y} = 1$$\n",
    "\n",
    "Now, if we ask about the gradient with respect to any of the inputs, we can readily use the chain rule,\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = z \\cdot 1$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y} = z\\cdot 1$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial z} = q $$\n",
    "\n",
    "\n",
    "This flow is the basic idea behind **automatic differentiation** or **algorithmic differentiation**. The idea is we can create a computational graph, and take advantage of this parsing to readily compute differentials (or store the operations involved) as we parse the graph. There are two flavours of automatic differentiation, **forward** and **backward**. Corresponding to two different views of operating using the chain rule, e.g. given the following chain\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial u}\\frac{\\partial q}{\\partial x},$$\n",
    "\n",
    "we can operate, from the start of the graph in a forward mode, this is\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\big(\\frac{\\partial q}{\\partial u}\\frac{\\partial q}{\\partial x}\\big),$$\n",
    "\n",
    "or backwards, starting from the end of the graph\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\big(\\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial u}\\big)\\frac{\\partial q}{\\partial x}.$$\n",
    "\n",
    "Forward autodiff is usually interesting when we are in front of a vectorial function and we need to compute its Jacobian, i.e. $$F(x) = (f_1(x),f_2(x), \\dots, f_N(x))$$ and we want to compute the Jacobian $$\\frac{\\partial F}{\\partial x} = (\\frac{\\partial f_1(x)}{\\partial x},\\dots, \\frac{\\partial f_N(x)}{\\partial x})$$.\n",
    "\n",
    "Backward differentiation is particularly useful when computing gradients, this is given a function $f(x_1,\\dots,x_N)$, its gradient is $$\\nabla f = (\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_N}).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class optimize:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, data, target, model, loss, num_iter, eta):\n",
    "        #Takes a loss function and a model and find the optimal parameters for an specific data set\n",
    "        N_samples = data.shape[0]\n",
    "        for t in xrange(num_iter):\n",
    "            #Step 1.-take a sample x at random from the training set\n",
    "            idx = np.random.randint(N_samples)\n",
    "            xi = data[idx,:]\n",
    "            yi = target[idx]\n",
    "            \n",
    "            #Step 2.- update the parameters to optimise\n",
    "            model.w = model.w - eta * loss.gradient(model,xi,yi)*model.gradient(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we need to add the gradient terms to the model and the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class model:\n",
    "    def __init__(self,d):\n",
    "        self.w = np.zeros((1,d+1))\n",
    "    def forward(self,x):\n",
    "        #Takes a data point and evaluates f(x,w)\n",
    "        return np.dot(self.w[0,:-1],x.T)+self.w[0,-1]\n",
    "    def gradient(self,x):\n",
    "        return np.concatenate((x,np.array([1])))\n",
    "    def gradient_x(self,x):\n",
    "        return self.w[0,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class loss:\n",
    "    def __init__(self):\n",
    "        self.y = 0\n",
    "        \n",
    "    def evaluate(self,model,x,t):\n",
    "        #Evaluates the loss function L(y,t)\n",
    "        self.y = model.forward(x)\n",
    "        return (t-self.y)*(t-self.y)\n",
    "    \n",
    "    def gradient(self,model,x,t):\n",
    "        self.y = model.forward(x)\n",
    "        return -2.*(t-self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us put all the pieces together and solve a toy problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Example dataset\n",
    "N_samples_per_class = 1000\n",
    "d_dimensions = 2\n",
    "x = np.vstack((np.random.randn(N_samples_per_class, d_dimensions),np.random.randn(N_samples_per_class, d_dimensions)+np.array([3,3])))\n",
    "y = np.vstack((-1.*np.ones((N_samples_per_class,1)),1.*np.ones((N_samples_per_class,1))))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = y==1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1)\n",
    "idx = y==-1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1,color='pink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 8.0, delta)\n",
    "yy = np.arange(-5.0, 8.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "test_data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "idx = y==1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1)\n",
    "idx = y==-1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1,color='pink')\n",
    "\n",
    "f=model(d_dimensions)\n",
    "Z = f.forward(test_data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-6,8,-6,8),alpha=0.3, vmin=-15, vmax=15)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add some control to check how it works. We will compute the loss every 1000 iterations, just for  checking purposes and add a plot method for showing convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class optimize:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, data, target, model, loss, num_iter, eta):\n",
    "        #Takes a loss function and a model and find the optimal parameters for an specific data set\n",
    "        self.l = np.zeros((np.ceil(num_iter/1000),1))\n",
    "        N_samples = data.shape[0]\n",
    "        print N_samples\n",
    "        i=0\n",
    "        for t in xrange(num_iter):\n",
    "            #Step 1.-take a sample x at random from the training set\n",
    "            idx = np.random.randint(N_samples)\n",
    "            xi = data[idx,:]\n",
    "            yi = target[idx]\n",
    "            # <-- Start new code\n",
    "            if t%1000==0:\n",
    "                self.l[i] = np.sum(loss.evaluate(model,data[:1000,:],target[:1000,0]))\n",
    "                i=i+1\n",
    "            ## End new code -->\n",
    "            #Step 2.- update the parameters to optimise\n",
    "            model.w = model.w - eta * loss.gradient(model,xi,yi)*model.gradient(xi)\n",
    "    def plot(self):\n",
    "        plt.plot(self.l)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us run the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_iter = 100000\n",
    "eta = 0.001 #optimization step/learning rate\n",
    "\n",
    "f = model(d_dimensions)\n",
    "L = loss()\n",
    "opt = optimize()\n",
    "opt.run(x,y,f,L,num_iter,eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visually check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 8.0, delta)\n",
    "yy = np.arange(-5.0, 8.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "test_data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "idx = y==1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1)\n",
    "idx = y==-1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1,color='pink')\n",
    "\n",
    "Z = f.forward(test_data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-6,8,-6,8),alpha=0.3, vmin=-15, vmax=15)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for fun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a new loss function more adequate for classification purposes. The hinge loss\n",
    "\n",
    "$\\mathcal{L}(y,t) = \\max(0, 1-t\\cdot y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display\n",
    "class loss_hinge:\n",
    "    def __init__(self):\n",
    "        self.y = 0\n",
    "        \n",
    "    def evaluate(self,model,x,t):\n",
    "        #Evaluates the loss function L(y,t)\n",
    "        self.y = model.forward(x)\n",
    "        return np.maximum(0,1-t*self.y)\n",
    "    \n",
    "    def gradient(self,model,x,t):\n",
    "        self.y = model.forward(x)\n",
    "        if (1-t*self.y)>0:\n",
    "            return -t\n",
    "        else:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_iter = 10000\n",
    "eta = 0.01 #optimization step/learning rate\n",
    "\n",
    "f = model(d_dimensions)\n",
    "L = loss_hinge()\n",
    "opt = optimize()\n",
    "opt.run(x,y,f,L,num_iter,eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 8.0, delta)\n",
    "yy = np.arange(-5.0, 8.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "test_data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "idx = y==1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.3)\n",
    "idx = y==-1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.3,color='red')\n",
    "\n",
    "Z = f.forward(test_data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-6,8,-6,8),alpha=0.3, vmin=-15, vmax=15)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even more fun\n",
    "\n",
    "We still do not have a perfect result. But we could modify the data set so that our classifier solves the problem perfectly. Remember that the gradient can be seen as what change we must made to a certain variable in order to maximize some cost function. What if the variable is data? This would mean how should we change data for a perfect classification?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a new optimizer in order to update data.\n",
    "\n",
    "<div class = \"alert alert-info\" style=\"border-radius:10px\">**EXERCISE: ** Discuss how can we change the problem to get a perfect classification. Fill the blank in the code to check it.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "from IPython.display import clear_output, Image, display\n",
    "%matplotlib inline\n",
    "\n",
    "class optimize_data:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, data, target, model, loss, num_iter, eta):\n",
    "        #Takes a loss function and a model and find the optimal parameters for an specific data set\n",
    "        N_samples = data.shape[0]\n",
    "        for t in xrange(num_iter):\n",
    "            \n",
    "\n",
    "            #Step 1.-take a sample x at random from the training set\n",
    "            idx = np.random.randint(N_samples)\n",
    "            xi = data[idx,:]\n",
    "            yi = target[idx,:]\n",
    "            \n",
    "            #Step 2.- update the parameters to optimise\n",
    "            data[idx,:] = data[idx,:] - eta * ##### YOUR CODE HERE ##############\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train a classifier first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_iter = 100000\n",
    "eta = 0.001 #optimization step/learning rate\n",
    "\n",
    "f = model(d_dimensions)\n",
    "L = loss_hinge()\n",
    "opt = optimize()\n",
    "opt.run(x,y,f,L,num_iter,eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the results and see that the result is good but does not classify all data perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 8.0, delta)\n",
    "yy = np.arange(-5.0, 8.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "test_data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "idx = y==1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1)\n",
    "idx = y==-1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1,color='pink')\n",
    "\n",
    "Z = f.forward(test_data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-6,8,-6,8),alpha=0.3, vmin=-15, vmax=15)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, modify the data so that the trained classifier classifies all data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_iter = 500000\n",
    "eta = 0.05 #optimization step/learning rate\n",
    "x_orig = x.copy()\n",
    "Lh = loss_hinge()\n",
    "opt = optimize_data()\n",
    "x_mod = opt.run(x_orig,y,f,Lh,num_iter,eta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 8.0, delta)\n",
    "yy = np.arange(-5.0, 8.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "test_data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "idx = y==1\n",
    "plt.scatter(x_mod[idx.ravel(),0],x_mod[idx.ravel(),1],alpha=0.4)\n",
    "idx = y==-1\n",
    "plt.scatter(x_mod[idx.ravel(),0],x_mod[idx.ravel(),1],alpha=0.4,color='red')\n",
    "\n",
    "Z = f.forward(test_data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-6,8,-6,8),alpha=0.3, vmin=-15, vmax=15)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep models\n",
    "\n",
    "Deep models are defined as the composition or stacking of functions. For example, consider the following graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pipeline2.png\" width = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "\n",
    "$$y_2 = f_2(f_1(x)).$$\n",
    "\n",
    "Observe that we are just changing the model while the loss and the optimization function remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different to the other case the parameters are arranged in layers. Thus the computation of the gradient with respect to the parameters is a little more involved. However, we will use standard chain rule. In this case we want to find \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\omega_2}} = \\frac{\\partial \\mathcal{L}}{\\partial y_2}\\frac{\\partial y_2}{\\partial \\bar{\\omega_2}}$$\n",
    "and\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\omega_1}} = \\frac{\\partial \\mathcal{L}}{\\partial y_2}\\frac{\\partial y_2}{\\partial \\bar{y_1}}\\frac{\\partial \\bar{y_1}}{\\partial \\bar{\\omega_1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that this decomposition allows to decouple each layer in the following terms\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\omega_1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\bar{y_1}}\\frac{\\partial \\bar{y_1}}{\\partial \\bar{\\omega_1}}$$\n",
    "\n",
    "In general for $N$ layers the update of the parameters of the $m$-th layer is written as \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\omega_m}} = \\frac{\\partial \\mathcal{L}}{\\partial \\bar{y_m}}\\frac{\\partial \\bar{y_m}}{\\partial \\bar{\\omega_m}} = \\frac{\\partial \\mathcal{L}}{\\partial \\bar{y_N}}\\prod\\limits_{i=m}^{N-1}\\frac{\\partial \\bar{y_{i+1}}}{\\partial \\bar{y_i}} \\frac{\\partial \\bar{y_m}}{\\partial \\bar{\\omega_m}}$$\n",
    "\n",
    "Thus we have to define for each layer two gradients:\n",
    "+ *the gradient with respect to the parameters* is the one used for updating the parameters\n",
    "+ *the gradient with respect to the layer input* is the one needed to move backward the gradient of the loss and will be used for updating lower layers.\n",
    "\n",
    "This is why it is called **backpropagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self): # evaluate the layer\n",
    "        pass\n",
    "    def backward(self): # gradient with respect to the inputs\n",
    "        pass\n",
    "    def gradient(self): # gradient with respect to the parameters\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class reluLayer(layer): #this is a linear layer with relu activation\n",
    "    def __init__(self,input_dim,n_neurons):\n",
    "        self.x = np.zeros((1,input_dim))\n",
    "        self.z = np.zeros((1,n_neurons))\n",
    "        self.w = np.random.randn(input_dim,n_neurons)\n",
    "        self.y = np.zeros((1,n_neurons))\n",
    "    def forward(self,x): # evaluate the layer\n",
    "        self.z = np.dot(x, self.w)\n",
    "        self.y = np.maximum(0,self.z)\n",
    "        return self.y\n",
    "    def backward(self): # gradient with respect to the inputs\n",
    "        return np.select([self.z>0],[self.w],default=0).T\n",
    "    def gradient(self): # gradient with respect to the parameters\n",
    "        dydz = np.where(self.z>0,1.,0.)\n",
    "        return np.dot(dydz[:,np.newaxis],self.x).T   \n",
    "\n",
    "class model:\n",
    "    def __init__(self):\n",
    "        self.architecture = []\n",
    "        self.y_ = []\n",
    "    def addLayer(self,layer):\n",
    "        self.architecture.append(layer)\n",
    "    def forward(self,x):\n",
    "        #Takes a data point and evaluates f(x,w)\n",
    "        self.y_= x\n",
    "        for layer in self.architecture:\n",
    "            self.y_=layer.forward(self.y_)\n",
    "        return self.y_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class optimize:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, data, target, model, loss, num_iter, eta):\n",
    "        #Takes a loss function and a model and find the optimal parameters for an specific data set\n",
    "        N_samples = data.shape[0]\n",
    "        for t in xrange(num_iter):\n",
    "            #Step 1.-take a sample x at random from the training set\n",
    "            idx = np.random.randint(N_samples)\n",
    "            xi = data[idx,:]\n",
    "            yi = target[idx,:]\n",
    "            \n",
    "            #Step 2.- update the parameters to optimise\n",
    "            dLdx= loss.gradient(model,xi,yi)[:,np.newaxis]\n",
    "            for layer in reversed(model.architecture):\n",
    "                tmp = layer.w - eta * np.dot(layer.gradient(),dLdx.T)\n",
    "                dLdx = np.dot(dLdx,layer.backward())\n",
    "                layer.w = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_iter = 10000\n",
    "eta = 0.01 #optimization step/learning rate\n",
    "nn = model()\n",
    "nn.addLayer(reluLayer(2,10))\n",
    "nn.addLayer(reluLayer(10,100))\n",
    "nn.addLayer(reluLayer(100,30))\n",
    "nn.addLayer(reluLayer(30,10))\n",
    "nn.addLayer(reluLayer(10,1))\n",
    "L = loss()\n",
    "opt = optimize()\n",
    "x_mod = opt.run(x,y,nn,L,num_iter,eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 8.0, delta)\n",
    "yy = np.arange(-5.0, 8.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "test_data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "idx = y==1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.3)\n",
    "idx = y==-1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.3,color='green')\n",
    "\n",
    "Z = nn.forward(test_data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-6,8,-6,8),alpha=0.3, vmin=-15, vmax=15)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class loss:\n",
    "    def __init__(self):\n",
    "        self.y = 0    \n",
    "    def evaluate(self,model,x,t):\n",
    "        #Evaluates the loss function L(y,t)\n",
    "        self.y = model.forward(x)\n",
    "        return (t-self.y)*(t-self.y)\n",
    "    def gradient(self,model,x,t):\n",
    "        self.y = model.forward(x)\n",
    "        return -2.*(t-self.y)\n",
    "    \n",
    "class reluLayer(layer): #this is a linear layer with relu activation\n",
    "    def __init__(self,input_dim,n_neurons):\n",
    "        self.x = np.zeros((1,input_dim))\n",
    "        self.z = np.zeros((1,n_neurons))\n",
    "        self.w = np.random.randn(input_dim,n_neurons) #Normal Random Initialization\n",
    "        self.y = np.zeros((1,n_neurons))\n",
    "    def forward(self,x): # evaluate the layer\n",
    "        self.z = np.dot(x, self.w)\n",
    "        self.y = np.maximum(0,self.z)\n",
    "        return self.y\n",
    "    def backward(self): # gradient with respect to the inputs\n",
    "        return np.select([self.z>0],[self.w],default=0).T\n",
    "    def gradient(self): # gradient with respect to the parameters\n",
    "        dydz = np.where(self.z>0,1.,0.)\n",
    "        return np.dot(dydz[:,np.newaxis],self.x).T   \n",
    "\n",
    "class model:\n",
    "    def __init__(self):\n",
    "        self.architecture = []\n",
    "        self.y_ = []\n",
    "    def addLayer(self,layer):\n",
    "        self.architecture.append(layer)\n",
    "    def forward(self,x):\n",
    "        #Takes a data point and evaluates f(x,w)\n",
    "        self.y_= x\n",
    "        for layer in self.architecture:\n",
    "            self.y_=layer.forward(self.y_)\n",
    "        return self.y_\n",
    "\n",
    "class optimize:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, data, target, model, loss, num_iter, eta):\n",
    "        #Takes a loss function and a model and find the optimal parameters for an specific data set\n",
    "        N_samples = data.shape[0]\n",
    "        for t in xrange(num_iter):\n",
    "            #Step 1.-take a sample x at random from the training set\n",
    "            idx = np.random.randint(N_samples)\n",
    "            xi = data[idx,:]\n",
    "            yi = target[idx,:]\n",
    "            \n",
    "            #Step 2.- update the parameters to optimise\n",
    "            dLdx= loss.gradient(model,xi,yi)[:,np.newaxis]\n",
    "            for layer in reversed(model.architecture):\n",
    "                layer.w = layer.w - eta * np.dot(layer.gradient(),dLdx.T)\n",
    "                dLdx = np.dot(dLdx,layer.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together now ... with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Example dataset\n",
    "N_samples_per_class = 1000\n",
    "d_dimensions = 2\n",
    "x = np.vstack((np.random.randn(N_samples_per_class, d_dimensions),np.random.randn(N_samples_per_class, d_dimensions)+np.array([3,3])))\n",
    "y = np.vstack((-1.*np.ones((N_samples_per_class,1)),1.*np.ones((N_samples_per_class,1))))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = y==1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1)\n",
    "idx = y==-1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.1,color='pink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "class loss:\n",
    "    def __init__(self):\n",
    "        self.y = 0    \n",
    "    def evaluate(self,model,x,t):\n",
    "        #Evaluates the loss function L(y,t)\n",
    "        self.y = model.forward(x)\n",
    "        return (t-self.y)*(t-self.y)\n",
    "    def gradient(self,model,x,t):\n",
    "        self.y = model.forward(x)\n",
    "        return -2.*(t-self.y)\n",
    "    \n",
    "class reluLayer(layer): #this is a linear layer with relu activation\n",
    "    def __init__(self,input_dim,n_neurons):\n",
    "        self.z = np.zeros((1,n_neurons))\n",
    "        self.w = np.random.randn(input_dim+1,n_neurons) \n",
    "        #self.w = self.w/np.dot(np.ones((input_dim+1,1)),np.sum(self.w*self.w,axis=0)[np.newaxis,:])\n",
    "        self.y = np.zeros((1,n_neurons))\n",
    "    def forward(self,x): # evaluate the layer\n",
    "        self.xext = np.concatenate((x,np.ones((x.shape[0],1))),axis=1)\n",
    "        self.z = np.dot(self.xext, self.w)\n",
    "        self.y = np.maximum(0,self.z)\n",
    "        return self.y\n",
    "    def backward(self): # gradient with respect to the inputs\n",
    "        dydz = np.tile(np.where(self.z>0.,1.,0.),(self.w.shape[0],1))\n",
    "        tw = dydz*self.w\n",
    "        return tw[:-1,:] \n",
    "    def gradient(self): # gradient with respect to the parameters\n",
    "        dydz = np.where(self.z>0.,1.,0.)\n",
    "        return np.dot(dydz.T,self.xext)\n",
    "    \n",
    "class linearLayer(layer): #this is a linear layer with relu activation\n",
    "    def __init__(self,input_dim,n_neurons):\n",
    "        self.w = np.random.randn(input_dim+1,n_neurons) \n",
    "        self.y = np.zeros((1,n_neurons))\n",
    "    def forward(self,x): # evaluate the layer\n",
    "        self.xext = np.concatenate((x,np.ones((x.shape[0],1))),axis=1)\n",
    "        self.y = np.dot(self.xext, self.w)\n",
    "        return self.y\n",
    "    def backward(self): # gradient with respect to the inputs\n",
    "        return self.w[:-1,:] \n",
    "    def gradient(self): # gradient with respect to the parameters\n",
    "        return self.xext\n",
    "\n",
    "class model:\n",
    "    def __init__(self):\n",
    "        self.architecture = []\n",
    "        self.y_ = []\n",
    "    def addLayer(self,layer):\n",
    "        self.architecture.append(layer)\n",
    "    def forward(self,x):\n",
    "        #Takes a data point and evaluates f(x,w)\n",
    "        self.y_= x\n",
    "        for layer in self.architecture:\n",
    "            self.y_=layer.forward(self.y_)\n",
    "        return self.y_\n",
    "\n",
    "class optimize:\n",
    "    def __init__(self, debug = False, plot_convergence = 10.):\n",
    "        self.debug_ = debug\n",
    "        self.pc = plot_convergence\n",
    "        \n",
    "    def run(self, data, target, model, loss, num_iter, eta):\n",
    "        #Takes a loss function and a model and find the optimal parameters for an specific data set\n",
    "        self.l = np.zeros((np.ceil((num_iter*1.0)/self.pc),1))\n",
    "        print self.l.shape\n",
    "        i=0\n",
    "        \n",
    "        N_samples = data.shape[0]\n",
    "        for t in tqdm.tqdm(xrange(num_iter)):\n",
    "            #Step 1.-take a sample x at random from the training set\n",
    "            idx = np.random.randint(N_samples)\n",
    "            xi = data[idx,:]\n",
    "            xi = xi[np.newaxis,:]\n",
    "            yi = target[idx,:]\n",
    "            \n",
    "            if (t*1.)%self.pc==0:\n",
    "                self.l[i] = np.sum(loss.evaluate(model,data,target))\n",
    "                i=i+1\n",
    "            \n",
    "            #Step 2.- update the parameters to optimise\n",
    "            dLdx= loss.gradient(model,xi,yi)\n",
    "            for layer in reversed(model.architecture):\n",
    "                if self.debug_:\n",
    "                    print \"xi\"+str(xi.shape)\n",
    "                    print \"g\"+str(layer.gradient().shape)\n",
    "                    print \"dLdx\"+str(dLdx.shape)\n",
    "                    print \"w\"+str(layer.w.shape)\n",
    "                    print \"b\"+str(layer.backward().shape)\n",
    "                tmp = layer.w - eta * np.dot(dLdx,layer.gradient()).T\n",
    "                dLdx = np.dot(dLdx,layer.backward().T)\n",
    "                layer.w = tmp\n",
    "    def plot(self):\n",
    "        plt.plot(self.l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_iter = 100000\n",
    "eta = 0.001 #optimization step/learning rate\n",
    "nn = model()\n",
    "nn.addLayer(reluLayer(2,5))\n",
    "nn.addLayer(reluLayer(5,5))\n",
    "nn.addLayer(linearLayer(5,1))\n",
    "L = loss()\n",
    "opt = optimize(plot_convergence=100)\n",
    "x_mod = opt.run(x,y,nn,L,num_iter,eta)\n",
    "opt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(L.evaluate(nn,x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(nn.forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 8.0, delta)\n",
    "yy = np.arange(-5.0, 8.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "test_data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "idx = y==1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.3)\n",
    "idx = y==-1\n",
    "plt.scatter(x[idx.ravel(),0],x[idx.ravel(),1],alpha=0.3,color='green')\n",
    "\n",
    "Z = nn.forward(test_data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-6,8,-6,8),alpha=0.3, vmin=-15, vmax=15)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_forward():\n",
    "    nn = model()\n",
    "    nn.addLayer(reluLayer(2,1))\n",
    "    w = np.ndarray((3,1), buffer=np.array([1.,2.,0.]))\n",
    "    nn.architecture[0].w = w\n",
    "    x = np.ndarray((1,2), buffer=np.array([-0.1,0.1]))\n",
    "    assert nn.forward(x)==0.1, nn.forward(x)\n",
    "    nn = model()\n",
    "    nn.addLayer(reluLayer(2,1))\n",
    "    w = np.ndarray((3,1), buffer=np.array([1.,2.,0.]))\n",
    "    nn.architecture[0].w = w\n",
    "    x = np.ndarray((1,2), buffer=np.array([0.1,-0.1]))\n",
    "    assert nn.forward(x)==0., nn.forward(x)\n",
    "    \n",
    "def test_gradient():\n",
    "    nn = model()\n",
    "    nn.addLayer(reluLayer(2,1))\n",
    "    w = np.ndarray((3,1), buffer=np.array([1.,2.,0.]))\n",
    "    nn.architecture[0].w = w\n",
    "    x = np.ndarray((1,2), buffer=np.array([-0.1,0.1]))\n",
    "    l = loss()\n",
    "    eg = np.ndarray((1,3), buffer=np.array([-0.1,0.1,1.]))\n",
    "    eb = np.ndarray((2,1), buffer=np.array([1.,2.]))\n",
    "    #Step 2.- update the parameters to optimise\n",
    "    dLdx= l.gradient(nn,x,y)\n",
    "    for layer in reversed(nn.architecture):\n",
    "        assert np.all(np.equal(layer.gradient(),eg)), layer.gradient()\n",
    "        assert np.all(np.equal(layer.backward(),eb)), layer.backward()\n",
    "\n",
    "        \n",
    "    \n",
    "def test_loss():\n",
    "    nn = model()\n",
    "    nn.addLayer(reluLayer(2,1))\n",
    "    w = np.ndarray((3,1), buffer=np.array([1.,2.,0.]))\n",
    "    nn.architecture[0].w = w\n",
    "    l = loss()\n",
    "    x = np.ndarray((1,2), buffer=np.array([-0.1,0.1]))\n",
    "    t=1.\n",
    "    assert l.evaluate(nn,x,t)==0.81, \"Expected 0.81, returned: \"+str(l.evaluate(nn,x,t))\n",
    "    assert l.gradient(nn,x,t)==-1.8, \"Expected -1.8, returned: \"+str(l.gradient(nn,x,t))\n",
    "\n",
    "test_forward()\n",
    "test_loss()\n",
    "test_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Behavior in general acyclic directed graphs structures\n",
    "\n",
    "In general, we may have a workflow that involves an acyclic directed graph. If that is the case we find two new elements where gradient is involved, splitting nodes and joint nodes. \n",
    "\n",
    "Consider an splitting node. How will backpropagation work in that case?\n",
    "\n",
    "<img src=\"images/split.png\" width=\"150\">\n",
    "\n",
    "Looking at the picture we realise that \n",
    "$$(y_1,y_2)=f(x,\\omega) = (f_1(x,\\omega),f_2(x,\\omega)).$$\n",
    "\n",
    "Remember that in backpropagation we need to define two derivatives, the update derivative $\\frac{\\partial \\mathcal{L}}{\\partial \\omega}$ and the backpropagation derivative $\\frac{\\partial \\mathcal{L}}{\\partial x}$.\n",
    "\n",
    "Remember that at any layer we are able to define these two quantities as soon as we have $\\frac{\\partial \\mathcal{L}}{\\partial \\bar{y}} = (\\frac{\\partial \\mathcal{L}}{\\partial y_1}, \\frac{\\partial \\mathcal{L}}{\\partial y_2})$. \n",
    "\n",
    "Again we use the differentiation chain rule, \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = \\frac{\\partial \\mathcal{L}}{\\partial \\bar{y}}^T\\frac{\\partial \\bar{y}}{\\partial \\omega}$$\n",
    "As stated in the former lines\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\bar{y}} = (\\frac{\\partial \\mathcal{L}}{\\partial y_1},\\frac{\\partial \\mathcal{L}}{\\partial y_2})^T$$\n",
    "and \n",
    "\n",
    "$$\\frac{\\partial \\bar{y}}{\\partial \\omega} = (\\frac{\\partial y_1}{\\partial \\omega},\\frac{\\partial y_2}{\\partial \\omega})^T$$\n",
    "\n",
    "Thus, \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = (\\frac{\\partial \\mathcal{L}}{\\partial y_1},\\frac{\\partial \\mathcal{L}}{\\partial y_2})^T \\left(\\begin{matrix}\\frac{\\partial y_1}{\\partial \\omega}\\\\ \\frac{\\partial y_2}{\\partial \\omega}\\end{matrix}\\right) = $$\n",
    "\n",
    "$$= \\frac{\\partial \\mathcal{L}}{\\partial y_1} \\frac{\\partial y_1}{\\partial \\omega} + \\frac{\\partial \\mathcal{L}}{\\partial y_2}\\frac{\\partial y_2}{\\partial \\omega}$$\n",
    "\n",
    "Observe that the influence of both gradients backpropagating through the graph is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
